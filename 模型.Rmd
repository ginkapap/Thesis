---
title: "Untitled"
author: "林茂廷"
date: "6/10/2019"
output: html_document
---

## 分類問題
對學生i，給定預測二一特徵$x_i$，要如何產生預測結果$\hat{y}_i$，不同分類工具可從「模型」、「Training方式」、及「預測方式」來區分說明。

## 模型

### Logistic regression
Model：
$$
p_i=\Pr(Y_i=1|X_i)=\frac{\exp(\beta'x_i)}{1+\exp(\beta'x_i)}
$$
Training：
$$
max_{\beta} \sum_{i\in \mathcal{S}_{train}} \{ y_i \log p_i+(1-y_i)\log(1-p_i) \}
$$

$$
\hat{p}_i=\frac{\exp(\hat{\beta}'x_i)}{1+\exp(\hat{\beta}'x_i)}
$$
預測法：
$$
 \hat{y}_i =
  \begin{cases}
    1       & \text{if } \hat{p}_i \geq \hat{c}\\
    0  & \text{otherwise.} 
  \end{cases}
$$

### 決策樹

Model：一連串的樹狀決策結構，從根結點出發開始一連串由單一特徵變數所形成的「是/否」分枝，一直下去直到判斷出所屬分類。

example:

"過去是否有二一" 

  * 是：預測會二一
  
  * 不：預測不會二一

Training:

  每一筆資料依決策樹分類後，會落在其中一個葉結點，因此每個葉結果會搜集到一群資料。完美的分類必需是每個葉結點資料群都是同類的，即都被二一或都不被二一，因此，對於一棵決策樹可計算它每個葉節點群的異類混雜程度，即不純度的概念，接著再進一步去考慮要不要對某個葉節點改成特徵變數子結點，進一步分類，以降低整體的不純度，直到不純度夠低為止。
  
## 支持向量機

Model： 
$$
f(x_i|w,b)=w^Tx_i+b
$$
預測法
$$
\hat{y}_i =
  \begin{cases}
    1       & \text{if } f(x_i|w,b)=w^Tx_i+b \geq 0\\
    0  & \text{otherwise.} 
  \end{cases}
$$

Training：
若training data可被特徵變數超平面完美區隔成2類，則為線性可分，其訓練過程在於：
$$
\begin{array}{lcl}
max_{w,b} & & margin(w,b)\\
s.t. & & y_i(w^Tx_i+b)>0 \mbox{ for }i\in\mathcal{S}_{train}
\end{array}
$$

若training data不可被特徵變數超平面完美區隔成2類，則為線性不可分，其訓練過程在於：

## 人工神經網路

Model：
$$
\begin{array}{lcl}
x_i(K\times1) &\stackrel{g}{\rightarrow} & n_i(M\times 1)\\
n_i(M\times 1) &\stackrel{f}{\rightarrow} & p_i(1\times 1)
\end{array}
$$
其中
$$
g(x_i)=
\begin{bmatrix}
g_1(x_i) \\
g_2(x_i) \\
\vdots \\
g_M(x_i)
\end{bmatrix}=
\begin{bmatrix}
a(w_1'x_i+b_1)\\
a(w_2'x_i+b_2)\\
\vdots\\
a(w_M'x_i+b_M)
\end{bmatrix}=
\begin{bmatrix}
n_{1,i}\\
n_{2,i}\\
\vdots\\
n_{M,i}
\end{bmatrix}=n_i
$$
函數a(z)一般稱為激活函數，用來控制z是否輸出，常用的函數型態為$a(z)=max(0,z)$; 此外, 我們選定$f(z)$為一logistic函數:
$$
f(n_i)=\frac{\exp(\beta'n_i)}{1+\exp(\beta'n_i)}
$$

Training：

$$
max_{\textbf{w,b},\beta} \sum_{i\in \mathcal{S}_{train}} \{ y_i \log p_i+(1-y_i)\log(1-p_i) \}
$$

預測法：
$$
 \hat{y}_i =
  \begin{cases}
    1       & \text{if } \hat{p}_i \geq \hat{c}\\
    0  & \text{otherwise.} 
  \end{cases}
$$
